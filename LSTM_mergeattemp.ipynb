{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "566ae220bf64df29451e5636809560ae174f6453"
      },
      "cell_type": "code",
      "source": "#####################################\n# Libraries\n#####################################\n# Common libs\nimport pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport os.path\nimport random\nfrom pathlib import Path\n\nfrom time import time\nfrom itertools import chain\n\n# Image processing\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\n#from skimage.transform import rescale, resize, downscale_local_mean\n\n# Charts\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\n\n\n# ML\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nfrom xgboost import XGBClassifier\n#from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n#from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import QuantileTransformer,StandardScaler, MinMaxScaler,OneHotEncoder, LabelEncoder, RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n#from sklearn.preprocessing import OneHotEncoder\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, LSTM, Embedding\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.utils import to_categorical\nimport tensorflow\n\n#####################################\n# Settings\n#####################################\nplt.style.use('seaborn')\n# Set random seed to make results reproducable\nnp.random.seed(42)\ntensorflow.set_random_seed(42)\nos.environ['PYTHONHASHSEED'] = '42'\n# Improve printed df readability\npd.options.display.float_format = '{:,.4f}'.format\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', 200)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from itertools import chain\nfrom kaggle.competitions import twosigmanews\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom wordcloud import WordCloud\n\nimport datetime\nimport gc\n\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport seaborn as sns\nimport warnings\n\n%matplotlib inline\nnp.random.seed(2018)\nstop = set(stopwords.words('english'))\npy.init_notebook_mode(connected=True)\nwarnings.filterwarnings('ignore')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "99126ff18d939122d6f768b5d17006fdbdf429a1"
      },
      "cell_type": "code",
      "source": "# turn off analytics for training, or running out of memory.\nANALYTICS = False",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "70b687da901d120c4244a15f13775ee59fe54d79"
      },
      "cell_type": "markdown",
      "source": "### Download Data"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "\nenv = twosigmanews.make_env()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1ee6824cf41c4fd03be113d54e6975cf3574c06f"
      },
      "cell_type": "code",
      "source": "(market_train_df, news_train_df) = env.get_training_data()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "00dd55351241ca2ab20f71ac77bc08c4e4e9e9e8"
      },
      "cell_type": "code",
      "source": "market_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "04fe6a44a65a7b66fa128f24acf6717eda1f6e20"
      },
      "cell_type": "code",
      "source": "market_train_df['close_to_open'] =  np.abs(market_train_df['close'] / market_train_df['open'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "scrolled": true,
        "_uuid": "19ef8496d92912fd56dce27ea0548c8a42c92212"
      },
      "cell_type": "code",
      "source": "market_train_df['assetName_mean_open'] = market_train_df.groupby('assetName')['open'].transform('mean')\nmarket_train_df['assetName_mean_close'] = market_train_df.groupby('assetName')['close'].transform('mean')\n\n# if open price is too far from mean open price for this company, replace it. Otherwise replace close price.\nfor i, row in market_train_df.loc[market_train_df['close_to_open'] >= 2].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df.iloc[i,4] = row['assetName_mean_close']\n        \nfor i, row in market_train_df.loc[market_train_df['close_to_open'] <= 0.5].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df.iloc[i,4] = row['assetName_mean_close']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "944ef1b8fa6e155248c487540c0a21888840dca9"
      },
      "cell_type": "code",
      "source": "market_train_df.drop(['price_diff', 'assetName_mean_open', 'assetName_mean_close'], axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0eec89a4d316f1268d205aba9d2c8972aef58a92"
      },
      "cell_type": "code",
      "source": "\nmarket_train_df['returnsOpenNextMktres10'] = market_train_df['returnsOpenNextMktres10'].clip(-0.2, 0.2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fa25da10ee626ce5f05544f11de4172dd2f4c8d8"
      },
      "cell_type": "code",
      "source": "market_train_df['returnsClosePrevRaw1'] = market_train_df['returnsClosePrevRaw1'].clip(-0.1, 0.1)\nmarket_train_df['returnsOpenPrevRaw1'] = market_train_df['returnsOpenPrevRaw1'].clip(-0.1, 0.1)\nmarket_train_df['returnsClosePrevMktres1'] = market_train_df['returnsClosePrevMktres1'].clip(-0.1, 0.1)\nmarket_train_df['returnsOpenPrevMktres1'] = market_train_df['returnsOpenPrevMktres1'].clip(-0.1, 0.1)\nmarket_train_df['returnsClosePrevRaw10'] = market_train_df['returnsClosePrevRaw10'].clip(-0.2, 0.2)\nmarket_train_df['returnsOpenPrevRaw10'] = market_train_df['returnsOpenPrevRaw10'].clip(-0.2, 0.2)\nmarket_train_df['returnsClosePrevMktres10'] = market_train_df['returnsClosePrevMktres10'].clip(-0.2, 0.2)\nmarket_train_df['returnsOpenPrevMktres10'] = market_train_df['returnsOpenPrevMktres10'].clip(-0.2, 0.2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8e784d89a6731a06bc75b5c0ded3730ec6d43701"
      },
      "cell_type": "markdown",
      "source": "\n\n## Modelling"
    },
    {
      "metadata": {
        "_uuid": "e859c650cecaebbfcb483291f82dd92b6bfe415e"
      },
      "cell_type": "markdown",
      "source": "\n\n\n\n\n\n\nThe VM doesn't have enough memory. Temporarily shrinking the dataset. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b9bfec74d70371f824b5de863776a7b94b69c594",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "market_train_df = market_train_df.tail(3000000)\nnews_train_df = news_train_df.tail(6000000)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "af69c00d180e5a23438fa0fc59d72868e9f7b1aa"
      },
      "cell_type": "markdown",
      "source": "\n\nFor market data, we extract more features from the original dataset directly. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "44c3af8a70c66887fbbf3b631e2c26922cc04c11"
      },
      "cell_type": "code",
      "source": "def extract_market_features(market_train_df):\n    # downcast to reduce memory footprint\n    market_train_df['volume'] = market_train_df['volume'].astype(np.float32)\n    market_train_df['open'] = market_train_df['open'].astype(np.float32)\n    market_train_df['close'] = market_train_df['close'].astype(np.float32)\n    market_train_df['returnsClosePrevRaw1'] = market_train_df['returnsClosePrevRaw1'].astype(np.float32)\n    market_train_df['returnsClosePrevRaw10'] = market_train_df['returnsClosePrevRaw10'].astype(np.float32)\n    market_train_df['returnsClosePrevMktres1'] = market_train_df['returnsClosePrevMktres1'].astype(np.float32)\n    market_train_df['returnsClosePrevMktres10'] = market_train_df['returnsClosePrevMktres10'].astype(np.float32)\n    market_train_df['returnsOpenPrevRaw1'] = market_train_df['returnsOpenPrevRaw1'].astype(np.float32)\n    market_train_df['returnsOpenPrevRaw10'] = market_train_df['returnsOpenPrevRaw10'].astype(np.float32)\n    market_train_df['returnsOpenPrevMktres1'] = market_train_df['returnsOpenPrevMktres1'].astype(np.float32)\n    market_train_df['returnsOpenPrevMktres10'] = market_train_df['returnsOpenPrevMktres10'].astype(np.float32)\n    \n    market_train_df['close_to_open'] = market_train_df['close'] / market_train_df['open']\n    market_train_df['close_to_open'] = market_train_df['close'] / market_train_df['open']\n    market_train_df['close_to_open'] = market_train_df['close'] / market_train_df['open']\n    market_train_df['close_to_open'] = market_train_df['close'] / market_train_df['open']\n    market_train_df['volume_to_mean'] = (market_train_df['volume'] / market_train_df['volume'].mean()).astype(np.float32)\n    market_train_df['returns_close_to_open_prev_raw1'] = market_train_df['returnsClosePrevRaw1'] / market_train_df['returnsOpenPrevRaw1']\n    market_train_df['returns_close_to_open_prev_raw10'] = market_train_df['returnsClosePrevRaw10'] / market_train_df['returnsOpenPrevRaw10']\n    market_train_df['returns_close_to_open_prev_mktres1'] = market_train_df['returnsClosePrevMktres1'] / market_train_df['returnsOpenPrevMktres1']\n    market_train_df['returns_close_to_open_prev_mktres10'] = market_train_df['returnsClosePrevMktres10'] / market_train_df['returnsOpenPrevMktres10']\n    market_train_df['returns_prev_open_raw1_to_close_raw10'] = market_train_df['returnsOpenPrevRaw1'] / market_train_df['returnsOpenPrevRaw10']\n    market_train_df['returns_prev_close_raw1_to_close_raw10'] = market_train_df['returnsClosePrevRaw1'] / market_train_df['returnsClosePrevRaw10']\n    market_train_df['returns_prev_open_mktres1_to_close_mktres10'] = market_train_df['returnsOpenPrevMktres1'] / market_train_df['returnsOpenPrevMktres10']\n    market_train_df['returns_prev_close_mktres1_to_close_mktres10'] = market_train_df['returnsClosePrevMktres1'] / market_train_df['returnsClosePrevMktres10']\n    \n    return market_train_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cdc6792191e4db0df58e839978050d7f0d19c670"
      },
      "cell_type": "markdown",
      "source": "\n\nFor news data, however, we may have multiple news articles associated with a stock on a given day. We collection a number of key statistics from these articles and aggregate them. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f715922487fcc7ea70f9ebf63841250ecb4a0889"
      },
      "cell_type": "code",
      "source": "def extract_news_features(news_train_df):\n    news_cols_agg = {\n        'urgency': ['min', 'max', 'count'],\n        'takeSequence': ['min', 'max', 'count'],\n        'bodySize': ['min', 'max', 'mean', 'std'],\n        'wordCount': ['min', 'max', 'mean', 'std'],\n        'sentenceCount': ['min', 'max', 'mean', 'std'],\n        'companyCount': ['min', 'max', 'mean', 'std'],\n        'marketCommentary': ['min', 'max', 'mean', 'std'],\n        'relevance': ['min', 'max', 'mean', 'std'],\n        'sentimentNegative': ['min', 'max', 'mean', 'std'],\n        'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n        'sentimentPositive': ['min', 'max', 'mean', 'std'],\n        'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n        'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n        'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n        'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n        'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n        'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n        'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n        'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n        'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n        'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n        'volumeCounts7D': ['min', 'max', 'mean', 'std']\n    }\n    \n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n    \n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    if(not news_train_df.empty): assetCodes_index = news_train_df.index.repeat(news_train_df['assetCodes'].apply(len)) \n    else: assetCodes_index = news_train_df.index\n    #assetCodes_index = news_train_df.index.repeat(news_train_df['assetCodes'].apply(len))\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    \n    # Free memory\n    del news_train_df_expanded\n\n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.astype(np.float32)\n    \n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n    \n    return news_train_df_aggregated",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "57973e95286b66b3f475859c110d12db37873609",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "\ndef merge_market_news(market_train_df, news_train_df):\n    df = market_train_df.join(news_train_df, on=['time', 'assetCode'])\n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef34295c713e03e30610e14aaeeda1fb782e75ee"
      },
      "cell_type": "code",
      "source": "def get_y(df):\n    label_cols = ['returnsOpenNextMktres10'] \n    real = df[label_cols] \n    y=(df[label_cols] >=0).astype(float)\n    return y,real",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e0f67ef5d6dc24c93f3d998782bad5f03a5f0d26"
      },
      "cell_type": "code",
      "source": "def get_xy(market_train_df, news_train_df,batch_idx, le=None):\n    X, le,t,u = get_x(market_train_df, news_train_df,batch_idx)\n    y,real = get_y(market_train_df)\n    return X, y, le,t,u,real\n\ndef label_encode(series, min_count):\n    vc = series.value_counts()\n    le = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n    return le\n\n\ndef get_x(market_train_df, news_train_df,batch_idx,le=None):\n    # Split date into before and after 22h (the time used in train data)\n    # E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n    #      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)\n    market_train_df = market_train_df.loc[batch_idx.index]\n    numeric_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                'returnsOpenPrevMktres10']\n    label_cols = ['returnsOpenNextMktres10']\n    market_train_df[numeric_cols] = market_train_df[ ['assetCode'] + numeric_cols].groupby('assetCode').transform(lambda g: g.fillna(method='bfill'))\n    scaler = preprocessing.StandardScaler()\n    market_train_df[numeric_cols] = scaler.fit_transform(market_train_df[numeric_cols]).astype(np.float32)\n    news_train_df = news_train_df.merge(batch_idx, on=['time'])\n    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n\n    # Round time of market_train_df to 0h of curret day\n    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n    market_train_df = extract_market_features(market_train_df)\n    news_train_df = extract_news_features(news_train_df)\n    X = merge_market_news(market_train_df, news_train_df)\n    universe = X['universe']\n    feature_cols = [ 'volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n                    'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10','close_to_open', \n                    'volume_to_mean', 'returns_close_to_open_prev_raw1', 'returns_close_to_open_prev_raw10', 'returns_close_to_open_prev_mktres1', 'returns_close_to_open_prev_mktres10', \n                    'returns_prev_open_raw1_to_close_raw10', 'returns_prev_close_raw1_to_close_raw10', 'returns_prev_open_mktres1_to_close_mktres10', \n                    'returns_prev_close_mktres1_to_close_mktres10']\n                    #, 'urgency_min', 'urgency_max', 'urgency_count', 'takeSequence_min', 'takeSequence_max', 'takeSequence_count', \n                    #'bodySize_min', 'bodySize_max', 'bodySize_mean', 'bodySize_std', 'wordCount_min', 'wordCount_max', 'wordCount_mean', 'wordCount_std', 'sentenceCount_min', \n                    #'sentenceCount_max', 'sentenceCount_mean', 'sentenceCount_std', 'companyCount_min', 'companyCount_max', 'companyCount_mean', 'companyCount_std', 'marketCommentary_min', \n                    #'marketCommentary_max', 'marketCommentary_mean', 'marketCommentary_std', 'relevance_min', 'relevance_max', 'relevance_mean', 'relevance_std', 'sentimentNegative_min', \n                    #'sentimentNegative_max', 'sentimentNegative_mean', 'sentimentNegative_std', 'sentimentNeutral_min', 'sentimentNeutral_max', 'sentimentNeutral_mean', \n                    #'sentimentNeutral_std', 'sentimentPositive_min', 'sentimentPositive_max', 'sentimentPositive_mean', 'sentimentPositive_std', 'sentimentWordCount_min', \n                    #'sentimentWordCount_max', 'sentimentWordCount_mean', 'sentimentWordCount_std', 'noveltyCount12H_min', 'noveltyCount12H_max', 'noveltyCount12H_mean',\n                    #'noveltyCount12H_std', 'noveltyCount24H_min', 'noveltyCount24H_max', 'noveltyCount24H_mean', 'noveltyCount24H_std', 'noveltyCount3D_min', 'noveltyCount3D_max', \n                    #'noveltyCount3D_mean', 'noveltyCount3D_std', 'noveltyCount5D_min', 'noveltyCount5D_max', 'noveltyCount5D_mean', 'noveltyCount5D_std', 'noveltyCount7D_min', \n                    #'noveltyCount7D_max', 'noveltyCount7D_mean', 'noveltyCount7D_std', 'volumeCounts12H_min', 'volumeCounts12H_max', 'volumeCounts12H_mean', 'volumeCounts12H_std', \n                    #'volumeCounts24H_min', 'volumeCounts24H_max', 'volumeCounts24H_mean', 'volumeCounts24H_std', 'volumeCounts3D_min', 'volumeCounts3D_max', 'volumeCounts3D_mean', \n                    #'volumeCounts3D_std', 'volumeCounts5D_min', 'volumeCounts5D_max', 'volumeCounts5D_mean', 'volumeCounts5D_std', 'volumeCounts7D_min', 'volumeCounts7D_max', 'volumeCounts7D_mean', 'volumeCounts7D_std']\n    X[feature_cols] = X[ ['assetCode'] + feature_cols].groupby('assetCode').transform(lambda g: g.fillna(method='bfill'))\n    scaler = preprocessing.StandardScaler()\n    X[feature_cols] = scaler.fit_transform(X[feature_cols]).astype(np.float32)\n    X['volume'].fillna(0,inplace=True)\n    X['close'].fillna(0,inplace=True)\n    X['open'].fillna(0,inplace=True)\n    X['returnsClosePrevRaw1'].fillna(0,inplace=True)\n    X['returnsOpenPrevRaw1'].fillna(0,inplace=True)\n    X['returnsClosePrevMktres1'].fillna(0,inplace=True)\n    X['returnsOpenPrevMktres1'].fillna(0,inplace=True)\n    X['returnsClosePrevRaw10'].fillna(0,inplace=True)\n    X['returnsOpenPrevRaw10'].fillna(0,inplace=True)\n    X['returnsClosePrevMktres10'].fillna(0,inplace=True)\n    X['returnsOpenPrevMktres10'].fillna(0,inplace=True)\n    X['universe'].fillna(0,inplace=True)\n    X['close_to_open'].fillna(0,inplace=True)\n    X['volume_to_mean'].fillna(0,inplace=True)\n    X['returns_close_to_open_prev_raw1'].fillna(0,inplace=True)\n    X['returns_close_to_open_prev_raw10'].fillna(0,inplace=True)\n    X['returns_close_to_open_prev_mktres1'].fillna(0,inplace=True)\n    X['returns_close_to_open_prev_mktres10'].fillna(0,inplace=True)\n    X['returns_prev_open_raw1_to_close_raw10'].fillna(0,inplace=True)\n    X['returns_prev_close_raw1_to_close_raw10'].fillna(0,inplace=True)\n    X['returns_prev_open_mktres1_to_close_mktres10'].fillna(0,inplace=True)\n    X['returns_prev_close_mktres1_to_close_mktres10'].fillna(0,inplace=True)   \n   # X['urgency_min'].fillna(0,inplace=True)\n   # X['urgency_max'].fillna(0,inplace=True)\n   # X['urgency_count'].fillna(0,inplace=True)\n  #  X['takeSequence_min'].fillna(0,inplace=True)\n   # X['takeSequence_max'].fillna(0,inplace=True)\n   # X['takeSequence_count'].fillna(0,inplace=True)\n   # X['bodySize_min'].fillna(0,inplace=True)\n   # X['bodySize_max'].fillna(0,inplace=True)\n   # X['bodySize_mean'].fillna(0,inplace=True)\n  #  X['bodySize_std'].fillna(0,inplace=True)\n   # X['wordCount_min'].fillna(0,inplace=True)\n   # X['wordCount_max'].fillna(0,inplace=True)\n    #X['wordCount_mean'].fillna(0,inplace=True)\n    #X['wordCount_std'].fillna(0,inplace=True)\n    #X['sentenceCount_min'].fillna(0,inplace=True)\n    #X['sentenceCount_max'].fillna(0,inplace=True)\n    #X['sentenceCount_mean'].fillna(0,inplace=True)\n   # X['sentenceCount_std'].fillna(0,inplace=True)\n  #  X['companyCount_min'].fillna(0,inplace=True)\n   # X['companyCount_max'].fillna(0,inplace=True)\n   # X['companyCount_mean'].fillna(0,inplace=True)\n    #X['companyCount_std'].fillna(0,inplace=True)\n    #X['marketCommentary_min'].fillna(0,inplace=True)\n    #X['marketCommentary_max'].fillna(0,inplace=True)\n    #X['marketCommentary_mean'].fillna(0,inplace=True)\n    #X['marketCommentary_std'].fillna(0,inplace=True)\n    #X['relevance_min'].fillna(0,inplace=True)\n    #X['relevance_max'].fillna(0,inplace=True)\n    #X['relevance_mean'].fillna(0,inplace=True)\n    ##X['relevance_std'].fillna(0,inplace=True)\n    #X['sentimentNegative_min'].fillna(0,inplace=True)\n   # X['sentimentNegative_max'].fillna(0,inplace=True)\n   #X['sentimentNegative_mean'].fillna(0,inplace=True)\n    #X['sentimentNegative_std'].fillna(0,inplace=True)\n    #X['sentimentNeutral_min'].fillna(0,inplace=True)\n    #X['sentimentNeutral_max'].fillna(0,inplace=True)\n    #X['sentimentNeutral_mean'].fillna(0,inplace=True)\n   # X['sentimentNeutral_std'].fillna(0,inplace=True)\n   # X['sentimentPositive_min'].fillna(0,inplace=True)\n   # X['sentimentPositive_max'].fillna(0,inplace=True)\n   # X['sentimentPositive_mean'].fillna(0,inplace=True)\n   # X['sentimentPositive_std'].fillna(0,inplace=True)\n   # X['sentimentWordCount_min'].fillna(0,inplace=True)\n   # X['sentimentWordCount_max'].fillna(0,inplace=True)\n   # X['sentimentWordCount_mean'].fillna(0,inplace=True)\n   # X['sentimentWordCount_std'].fillna(0,inplace=True)\n    #X['noveltyCount12H_min'].fillna(0,inplace=True)\n    #X['noveltyCount12H_max'].fillna(0,inplace=True)\n   # X['noveltyCount12H_mean'].fillna(0,inplace=True)\n   # X['noveltyCount12H_std'].fillna(0,inplace=True)\n   # X['noveltyCount24H_min'].fillna(0,inplace=True)\n   # X['noveltyCount24H_max'].fillna(0,inplace=True)\n   # X['noveltyCount24H_mean'].fillna(0,inplace=True)\n   # X['noveltyCount24H_std'].fillna(0,inplace=True)\n   # X['noveltyCount3D_min'].fillna(0,inplace=True)\n   # X['noveltyCount3D_max'].fillna(0,inplace=True)\n   # X['noveltyCount3D_mean'].fillna(0,inplace=True)\n   # X['noveltyCount3D_std'].fillna(0,inplace=True)\n   # X['noveltyCount5D_min'].fillna(0,inplace=True)\n   # X['noveltyCount5D_max'].fillna(0,inplace=True)\n   # X['noveltyCount5D_mean'].fillna(0,inplace=True)\n   # X['noveltyCount5D_std'].fillna(0,inplace=True)\n   # X['noveltyCount7D_min'].fillna(0,inplace=True)\n   # X['noveltyCount7D_max'].fillna(0,inplace=True)\n   # X['noveltyCount7D_mean'].fillna(0,inplace=True)\n   # X['noveltyCount7D_std'].fillna(0,inplace=True)\n   # X['volumeCounts12H_min'].fillna(0,inplace=True)\n   # X['volumeCounts12H_max'].fillna(0,inplace=True)\n   # X['volumeCounts12H_mean'].fillna(0,inplace=True)\n   # X['volumeCounts12H_std'].fillna(0,inplace=True)\n   # X['volumeCounts24H_min'].fillna(0,inplace=True)\n   # X['volumeCounts24H_max'].fillna(0,inplace=True)\n   # X['volumeCounts24H_mean'].fillna(0,inplace=True)\n   # X['volumeCounts24H_std'].fillna(0,inplace=True)\n    #X['volumeCounts3D_min'].fillna(0,inplace=True)\n    #X['volumeCounts3D_max'].fillna(0,inplace=True)\n    #X['volumeCounts3D_mean'].fillna(0,inplace=True)\n    #X['volumeCounts3D_std'].fillna(0,inplace=True)\n    #X['volumeCounts5D_min'].fillna(0,inplace=True)\n    #X['volumeCounts5D_max'].fillna(0,inplace=True)\n    #X['volumeCounts5D_mean'].fillna(0,inplace=True)\n    #X['volumeCounts5D_std'].fillna(0,inplace=True)\n    #X['volumeCounts7D_min'].fillna(0,inplace=True)\n    #X['volumeCounts7D_max'].fillna(0,inplace=True)\n    #X['volumeCounts7D_mean'].fillna(0,inplace=True)\n    #X['volumeCounts7D_std'].fillna(0,inplace=True)\n    \n    #print(X.isnull().any())\n   # If not label-encoder... encode assetCode\n   \n    if le is None:\n        le_assetCode = label_encode(X['assetCode'], min_count=10)\n        le_assetName = label_encode(X['assetName'], min_count=5)\n    else:\n        # 'unpack' label encoders\n        le_assetCode, le_assetName = le\n        \n    X['assetCode'] = X['assetCode'].map(le_assetCode).fillna(-1).astype(np.int16)\n    X['assetName'] = X['assetName'].map(le_assetName).fillna(-1).astype(np.int16)\n    time_batch = X['time'].dt.day.astype(np.int8)\n    \n    try:\n        X.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n    except:\n        pass\n    try:\n        X.drop(columns=['universe'], inplace=True)\n    except:\n        pass\n    X['dayofweek'], X['month'] = X.time.dt.dayofweek.astype(np.int8), X.time.dt.month.astype(np.int8)\n    X.drop(columns='time', inplace=True)\n\n    # Fix some mixed-type columns\n    for bogus_col in ['marketCommentary_min', 'marketCommentary_max']:\n        X[bogus_col] = X[bogus_col].astype(np.float32)\n    \n    features = X[feature_cols]\n    return features, (le_assetCode, le_assetName),time_batch,universe",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d3c741c3ad43c40d0be7acb177c11b4303337d89"
      },
      "cell_type": "markdown",
      "source": "\nWe then merge market data and news data, and remove the original dataframes to reduce memory footprint."
    },
    {
      "metadata": {
        "_uuid": "6857e16b9f52eb381b7282362c0de83885a1c943"
      },
      "cell_type": "markdown",
      "source": "Create training and validation datasets and add time and universe to validation dataset in order to calculate custom scores later. "
    },
    {
      "metadata": {
        "_uuid": "6dd1dae6fe0c664a1dd9ca4f9ee21a26eac370d1"
      },
      "cell_type": "markdown",
      "source": "\nFor LSTM dataset"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2b529b97b2ff02d8f40eefbfada4b313c0f9dd6",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "market_train_df = market_train_df.sort_values(by=['time'])\ntrain_index, test_index = train_test_split(market_train_df.index,shuffle=False, test_size=0.1, random_state=2018)\ntrain_index, valid_index = train_test_split(train_index,shuffle=False,test_size=0.2, random_state=2018)\nmarket_train_idx = market_train_df.loc[train_index][['time', 'assetCode']]\nmarket_val_idx = market_train_df.loc[valid_index][['time', 'assetCode']]\nmarket_test_idx = market_train_df.loc[test_index][['time', 'assetCode']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8276af116a6dd8b0636cf689e9bed492a9fd2b0c",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "class LSTM_Generator:\n    \n    def __init__(self,market, news, idx):\n        self.market = market\n        self.news = news\n        self.idx = idx\n\n    def lstm_batch_lookback(self, batch_size, is_train, look_back, look_back_step):\n        while True:\n            # Get market indices of random assets, sorted by assetCode, time.\n            batch_index_df = self.get_batch_idx(batch_size)\n            X, y, le,t,u,real= self.get_batch(batch_index_df)\n            X, y,index = self.with_look_back(X,y,t,u,real,look_back,look_back_step)\n            print(X,y)\n            yield X,y\n            \n    def get_batch_idx(self, batch_size):\n        asset_codes = self.idx['assetCode'].unique().tolist()\n        asset = np.random.choice(asset_codes)\n        asset_codes.remove(asset)\n        batch_index_df = self.idx[self.idx.assetCode == asset].tail(batch_size)\n        # Repeat until reach batch_size records\n        while (batch_index_df.index.size < batch_size) and (len(asset_codes) > 0):\n            asset = np.random.choice(asset_codes)\n            asset_codes.remove(asset)\n            asset_index_df = self.idx[self.idx.assetCode == asset].tail(batch_size - batch_index_df.index.size)\n            batch_index_df = pd.concat([batch_index_df, asset_index_df])\n        return batch_index_df.sort_values(by=['assetCode', 'time'])\n            \n    def get_batch(self, batch_idx):\n        X, y, le,t,u,real = get_xy(self.market, self.news, batch_idx)\n        return(X, y, le,t,u,real)\n    \n    def with_look_back(self, X, y, t, u,real,look_back, look_back_step):\n        \"\"\"\n        Add look back window values to prepare dataset for LSTM\n        \"\"\"\n        X_processed, y_processed = [], []\n        t_processed, u_processed = [], []\n        index_processed =[]\n        # Fix last window in batch, can be not full\n        \n        if look_back > len(X): \n            look_back = len(X)\n            look_back_step = min(look_back_step, look_back)\n            \n        for i in range(0,len(X)-look_back+1):\n            # Add lookback to X\n            x_window = X.values[i:(i+look_back):look_back_step, :]\n            X_processed.append(x_window)\n            if y is None: continue\n            y_window = y.values[i+look_back-1]\n            y_processed.append(y_window)\n            #t_window = t.values[i+look_back-1]\n            #t_processed.append(t_window)\n            #u_window = u.values[i+look_back-1]\n            #u_processed.append(u_window)\n            #r_window = real.values[i+look_back-1]\n            #real_processed.append(r_window)\n            index = X.index[i+look_back-1]\n            index_processed.append(index)\n        if(y is not None): return np.array(X_processed), np.array(y_processed),index_processed\n        else: return np.array(X_processed)\n\n    \ntrain_generator = LSTM_Generator(market_train_df, news_train_df,market_train_idx)\nval_generator = LSTM_Generator(market_train_df, news_train_df, market_val_idx)\ntest_generator = LSTM_Generator(market_train_df, news_train_df, market_test_idx)\nprint('Generators created')\n\n##X,y=next(train_generator.lstm_batch_lookback(20,True,10,2))\n##print(X)\n##print(y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0dc8b6b6b728dcb98c73cc8aa370430f1866969e"
      },
      "cell_type": "code",
      "source": "X,y=next(train_generator.lstm_batch_lookback(100,True,10,2))\nprint(X.shape)\nprint(y.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e6ebcb21e5312ababa398439a684d1ba51db62f6",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from keras.layers.advanced_activations import LeakyReLU\ndef lstm_128():\n    model = Sequential()\n    input_size = 21\n    model.add(LSTM(units=128, return_sequences=True, input_shape=(None,input_size)))\n    model.add(LSTM(units=64, return_sequences=True ))\n    model.add(LSTM(units=32, return_sequences=False))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n    return(model)        \n\nmodel = lstm_128()\nmodel.summary()\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab2ab24a06189ef1448ba1d71ae5e12442a5b694",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "batch_size=1000\nvalidation_batch_size=1000\nsteps_per_epoch=20\nvalidation_steps=5\nepochs=5\nlook_back=90\nlook_back_step=10\n\nprint(f'epochs:{epochs}, steps per epoch: {steps_per_epoch}, validation steps:{validation_steps}')\nprint(f'Batch_size:{batch_size}, validation batch size:{validation_batch_size}')\n\n# Fit\ntraining = model.fit_generator(train_generator.lstm_batch_lookback(batch_size=batch_size \n            , is_train=True \n            , look_back=look_back \n            , look_back_step=look_back_step) \n        , epochs=epochs \n        , validation_data=val_generator.lstm_batch_lookback(batch_size=validation_batch_size\n            , is_train=False\n            , look_back=look_back\n            , look_back_step=look_back_step) \n        , steps_per_epoch=steps_per_epoch \n        , validation_steps=validation_steps)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f103e12873f0140a5d2f7a5bcccca82c3336a89c"
      },
      "cell_type": "code",
      "source": "print(t_test.loc[idx])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "052d740ba5028bdcfc67f412018c2415d9a06e84"
      },
      "cell_type": "code",
      "source": "batch_idx = market_test_idx.index[0:999]\nmarket_sub_idx = market_train_df.loc[batch_idx][['time', 'assetCode']]\nasset = market_sub_idx['assetCode'].values[0]\nasset_idx = market_test_idx[market_test_idx.assetCode == asset]\nmarket_df = market_train_df.loc[market_train_df.assetCode == asset].copy().set_index(['time'], drop=False)\nnews_df = news_train_df.merge(market_df, on=['time'])\nX_test, y_test, le_test,t_test,u_test,real= get_xy(market_df,news_df,market_sub_idx)\nX_test, y_test,index= test_generator.with_look_back(X_test, y_test,t_test,u_test,real, look_back = 90, look_back_step=10)\nconfidence = np.clip(model.predict(X_test)*2-1,-1,1)\ny_pred = pd.DataFrame(confidence, index = market_df.iloc[90-1:]['time'].dt.date)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "a24f82549f8f71acc59fb15f4334cf1b0710d848"
      },
      "cell_type": "code",
      "source": "x_t_i = []\nt_pro = []\nfor i in range(0,300):\n    batch_idx = market_test_idx.index[i*1000:(i+1)*1000-1]\n    market_sub_idx = market_train_df.loc[batch_idx][['time', 'assetCode']]\n    X_test, y_test, le_test,t_test,u_test,real= get_xy(market_train_df,news_train_df,market_sub_idx)\n    X_test, y_test,index= test_generator.with_look_back(X_test, y_test,t_test,u_test,real, look_back = 90, look_back_step=10)\n    confidence = np.clip(model.predict(X_test)*2-1,-1,1)\n    y_pred = pd.DataFrame(confidence, index = index)\n    for idx in index:\n        x_t = y_pred.loc[idx].values * real.loc[idx].values\n        x_t = x_t * u_test.loc[idx]\n        x_t_i.append(x_t)\n        t_pro.append(t_test.loc[idx])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "21fe67cb14ad593659a41f1d817c020e9ca7f3a4"
      },
      "cell_type": "code",
      "source": "dataset = pd.DataFrame({'day': t_pro, 'x_t_i': x_t_i}, columns=['day', 'x_t_i'])\nx_t = dataset.groupby('day').sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b79e9cf85b2c922576e2b58dc850ee9cf32e1546"
      },
      "cell_type": "code",
      "source": "\nprint(y_pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "69a7e8adc4416d0dbe7b5db357d86be33384f299"
      },
      "cell_type": "code",
      "source": "x_t = dataset.groupby('day').sum()\nprint(x_t)\nmean = x_t.mean()\nstd =  x_t.std()\nscore = mean / std\nprint(score)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "196012dbf5c811233ee6d149ee06d28bcf15ca7d"
      },
      "cell_type": "code",
      "source": "#batch_idx = market_test_idx.index\n#market_sub_idx = market_train_df.loc[batch_idx][['time', 'assetCode']]\nprint(market_test_idx.index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a048ab83cb2adf6ead54525c51fc4547edadd3e9",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "print('The benchmark\\'s score on test data: ', evaluate(y_pred.loc[index].values, y_pred.loc[idx].values, \n                                                       u_test.loc[index].values, t_test.loc[index].values))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "13cb1360f46bc159a90def05a4626443bf36f201"
      },
      "cell_type": "code",
      "source": "def evaluate(preds, labels, universe, time):\n    x_t = preds * labels * universe\n    print(x_t.flatten().sum())\n    #dataset = pd.DataFrame({'day': np.array(time), 'x_t_i': x_t}, columns=['day', 'x_t_i'])\n    x_t_sum = x_t.flatten().sum()\n    print(x_t_sum.mean())\n    print(x_t_sum.std())\n    score = x_t_sum.mean() / x_t_sum.std()\n    return score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f91807595d4e4df7d88c58bdd05b7f6fde744028"
      },
      "cell_type": "code",
      "source": "def predict_on_test():\n    idx = market_test_idx\n    X_test, y_test, le_test,t_test,u_test = get_xy(market_train_df,news_train_df,idx)\n    X_test, y_test,t_test,u_test = test_generator.with_look_back(X_test, y_test,t_test,u_test, look_back = 90, look_back_step=10)\n    confidence = model.predict(X_test)*2-1\n    real_processed ,universe_processed =[],[]\n    for i in range(0,len(market_df)-90+1):\n        r=market_df['returnsOpenNextMktres10'].values[i+90-1]\n        real_processed.append(r)\n        u=market_df['universe'].values[i+90-1]\n        universe_processed.append(u)\n    \n    x_t_i = []\n    for i in range(0,len(real_processed)):\n        x_t = confidence[i] * real_processed[i] * universe_processed[i]\n        x_t_i.append(x_t)\n    \n    dataset = pd.DataFrame({'day': t_test, 'x_t_i': np.array(x_t_i).flatten()}, columns=['day', 'x_t_i'])\n    x_t = dataset.groupby('day').sum().values.flatten()\n    mean = np.mean(x_t)\n    std = np.std(x_t)\n    score = mean / std\n    print(score)\n    \npredict_on_test()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5f833d6e00d89ed607b4905b1f9da2cf8c4792c"
      },
      "cell_type": "code",
      "source": "def get_xy(market_train_df, news_train_df, le=None):\n    X, le,t,u = get_x(market_train_df, news_train_df)\n    y,real = get_y(market_train_df)\n    return X, y, le,t,u,real\n\ndef label_encode(series, min_count):\n    vc = series.value_counts()\n    le = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n    return le\n\n\ndef get_x(market_train_df, news_train_df,le=None):\n    # Split date into before and after 22h (the time used in train data)\n    # E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n    #      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)\n    numeric_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                'returnsOpenPrevMktres10']\n    label_cols = ['returnsOpenNextMktres10']\n    market_train_df[numeric_cols] = market_train_df[ ['assetCode'] + numeric_cols].groupby('assetCode').transform(lambda g: g.fillna(method='bfill'))\n    scaler = preprocessing.StandardScaler()\n    market_train_df[numeric_cols] = scaler.fit_transform(market_train_df[numeric_cols]).astype(np.float32)\n    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n\n    # Round time of market_train_df to 0h of curret day\n    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n    market_train_df = extract_market_features(market_train_df)\n    news_train_df = extract_news_features(news_train_df)\n    X = merge_market_news(market_train_df, news_train_df)\n    universe = X['universe']\n    feature_cols = [ 'volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n                    'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10','close_to_open', \n                    'volume_to_mean', 'returns_close_to_open_prev_raw1', 'returns_close_to_open_prev_raw10', 'returns_close_to_open_prev_mktres1', 'returns_close_to_open_prev_mktres10', \n                    'returns_prev_open_raw1_to_close_raw10', 'returns_prev_close_raw1_to_close_raw10', 'returns_prev_open_mktres1_to_close_mktres10', \n                    'returns_prev_close_mktres1_to_close_mktres10']\n                    #, 'urgency_min', 'urgency_max', 'urgency_count', 'takeSequence_min', 'takeSequence_max', 'takeSequence_count', \n                    #'bodySize_min', 'bodySize_max', 'bodySize_mean', 'bodySize_std', 'wordCount_min', 'wordCount_max', 'wordCount_mean', 'wordCount_std', 'sentenceCount_min', \n                    #'sentenceCount_max', 'sentenceCount_mean', 'sentenceCount_std', 'companyCount_min', 'companyCount_max', 'companyCount_mean', 'companyCount_std', 'marketCommentary_min', \n                    #'marketCommentary_max', 'marketCommentary_mean', 'marketCommentary_std', 'relevance_min', 'relevance_max', 'relevance_mean', 'relevance_std', 'sentimentNegative_min', \n                    #'sentimentNegative_max', 'sentimentNegative_mean', 'sentimentNegative_std', 'sentimentNeutral_min', 'sentimentNeutral_max', 'sentimentNeutral_mean', \n                    #'sentimentNeutral_std', 'sentimentPositive_min', 'sentimentPositive_max', 'sentimentPositive_mean', 'sentimentPositive_std', 'sentimentWordCount_min', \n                    #'sentimentWordCount_max', 'sentimentWordCount_mean', 'sentimentWordCount_std', 'noveltyCount12H_min', 'noveltyCount12H_max', 'noveltyCount12H_mean',\n                    #'noveltyCount12H_std', 'noveltyCount24H_min', 'noveltyCount24H_max', 'noveltyCount24H_mean', 'noveltyCount24H_std', 'noveltyCount3D_min', 'noveltyCount3D_max', \n                    #'noveltyCount3D_mean', 'noveltyCount3D_std', 'noveltyCount5D_min', 'noveltyCount5D_max', 'noveltyCount5D_mean', 'noveltyCount5D_std', 'noveltyCount7D_min', \n                    #'noveltyCount7D_max', 'noveltyCount7D_mean', 'noveltyCount7D_std', 'volumeCounts12H_min', 'volumeCounts12H_max', 'volumeCounts12H_mean', 'volumeCounts12H_std', \n                    #'volumeCounts24H_min', 'volumeCounts24H_max', 'volumeCounts24H_mean', 'volumeCounts24H_std', 'volumeCounts3D_min', 'volumeCounts3D_max', 'volumeCounts3D_mean', \n                    #'volumeCounts3D_std', 'volumeCounts5D_min', 'volumeCounts5D_max', 'volumeCounts5D_mean', 'volumeCounts5D_std', 'volumeCounts7D_min', 'volumeCounts7D_max', 'volumeCounts7D_mean', 'volumeCounts7D_std']\n    X[feature_cols] = X[ ['assetCode'] + feature_cols].groupby('assetCode').transform(lambda g: g.fillna(method='bfill'))\n    scaler = preprocessing.StandardScaler()\n    X[feature_cols] = scaler.fit_transform(X[feature_cols]).astype(np.float32)\n    X['volume'].fillna(0,inplace=True)\n    X['close'].fillna(0,inplace=True)\n    X['open'].fillna(0,inplace=True)\n    X['returnsClosePrevRaw1'].fillna(0,inplace=True)\n    X['returnsOpenPrevRaw1'].fillna(0,inplace=True)\n    X['returnsClosePrevMktres1'].fillna(0,inplace=True)\n    X['returnsOpenPrevMktres1'].fillna(0,inplace=True)\n    X['returnsClosePrevRaw10'].fillna(0,inplace=True)\n    X['returnsOpenPrevRaw10'].fillna(0,inplace=True)\n    X['returnsClosePrevMktres10'].fillna(0,inplace=True)\n    X['returnsOpenPrevMktres10'].fillna(0,inplace=True)\n    X['universe'].fillna(0,inplace=True)\n    X['close_to_open'].fillna(0,inplace=True)\n    X['volume_to_mean'].fillna(0,inplace=True)\n    X['returns_close_to_open_prev_raw1'].fillna(0,inplace=True)\n    X['returns_close_to_open_prev_raw10'].fillna(0,inplace=True)\n    X['returns_close_to_open_prev_mktres1'].fillna(0,inplace=True)\n    X['returns_close_to_open_prev_mktres10'].fillna(0,inplace=True)\n    X['returns_prev_open_raw1_to_close_raw10'].fillna(0,inplace=True)\n    X['returns_prev_close_raw1_to_close_raw10'].fillna(0,inplace=True)\n    X['returns_prev_open_mktres1_to_close_mktres10'].fillna(0,inplace=True)\n    X['returns_prev_close_mktres1_to_close_mktres10'].fillna(0,inplace=True)   \n   # X['urgency_min'].fillna(0,inplace=True)\n   # X['urgency_max'].fillna(0,inplace=True)\n   # X['urgency_count'].fillna(0,inplace=True)\n  #  X['takeSequence_min'].fillna(0,inplace=True)\n   # X['takeSequence_max'].fillna(0,inplace=True)\n   # X['takeSequence_count'].fillna(0,inplace=True)\n   # X['bodySize_min'].fillna(0,inplace=True)\n   # X['bodySize_max'].fillna(0,inplace=True)\n   # X['bodySize_mean'].fillna(0,inplace=True)\n  #  X['bodySize_std'].fillna(0,inplace=True)\n   # X['wordCount_min'].fillna(0,inplace=True)\n   # X['wordCount_max'].fillna(0,inplace=True)\n    #X['wordCount_mean'].fillna(0,inplace=True)\n    #X['wordCount_std'].fillna(0,inplace=True)\n    #X['sentenceCount_min'].fillna(0,inplace=True)\n    #X['sentenceCount_max'].fillna(0,inplace=True)\n    #X['sentenceCount_mean'].fillna(0,inplace=True)\n   # X['sentenceCount_std'].fillna(0,inplace=True)\n  #  X['companyCount_min'].fillna(0,inplace=True)\n   # X['companyCount_max'].fillna(0,inplace=True)\n   # X['companyCount_mean'].fillna(0,inplace=True)\n    #X['companyCount_std'].fillna(0,inplace=True)\n    #X['marketCommentary_min'].fillna(0,inplace=True)\n    #X['marketCommentary_max'].fillna(0,inplace=True)\n    #X['marketCommentary_mean'].fillna(0,inplace=True)\n    #X['marketCommentary_std'].fillna(0,inplace=True)\n    #X['relevance_min'].fillna(0,inplace=True)\n    #X['relevance_max'].fillna(0,inplace=True)\n    #X['relevance_mean'].fillna(0,inplace=True)\n    ##X['relevance_std'].fillna(0,inplace=True)\n    #X['sentimentNegative_min'].fillna(0,inplace=True)\n   # X['sentimentNegative_max'].fillna(0,inplace=True)\n   #X['sentimentNegative_mean'].fillna(0,inplace=True)\n    #X['sentimentNegative_std'].fillna(0,inplace=True)\n    #X['sentimentNeutral_min'].fillna(0,inplace=True)\n    #X['sentimentNeutral_max'].fillna(0,inplace=True)\n    #X['sentimentNeutral_mean'].fillna(0,inplace=True)\n   # X['sentimentNeutral_std'].fillna(0,inplace=True)\n   # X['sentimentPositive_min'].fillna(0,inplace=True)\n   # X['sentimentPositive_max'].fillna(0,inplace=True)\n   # X['sentimentPositive_mean'].fillna(0,inplace=True)\n   # X['sentimentPositive_std'].fillna(0,inplace=True)\n   # X['sentimentWordCount_min'].fillna(0,inplace=True)\n   # X['sentimentWordCount_max'].fillna(0,inplace=True)\n   # X['sentimentWordCount_mean'].fillna(0,inplace=True)\n   # X['sentimentWordCount_std'].fillna(0,inplace=True)\n    #X['noveltyCount12H_min'].fillna(0,inplace=True)\n    #X['noveltyCount12H_max'].fillna(0,inplace=True)\n   # X['noveltyCount12H_mean'].fillna(0,inplace=True)\n   # X['noveltyCount12H_std'].fillna(0,inplace=True)\n   # X['noveltyCount24H_min'].fillna(0,inplace=True)\n   # X['noveltyCount24H_max'].fillna(0,inplace=True)\n   # X['noveltyCount24H_mean'].fillna(0,inplace=True)\n   # X['noveltyCount24H_std'].fillna(0,inplace=True)\n   # X['noveltyCount3D_min'].fillna(0,inplace=True)\n   # X['noveltyCount3D_max'].fillna(0,inplace=True)\n   # X['noveltyCount3D_mean'].fillna(0,inplace=True)\n   # X['noveltyCount3D_std'].fillna(0,inplace=True)\n   # X['noveltyCount5D_min'].fillna(0,inplace=True)\n   # X['noveltyCount5D_max'].fillna(0,inplace=True)\n   # X['noveltyCount5D_mean'].fillna(0,inplace=True)\n   # X['noveltyCount5D_std'].fillna(0,inplace=True)\n   # X['noveltyCount7D_min'].fillna(0,inplace=True)\n   # X['noveltyCount7D_max'].fillna(0,inplace=True)\n   # X['noveltyCount7D_mean'].fillna(0,inplace=True)\n   # X['noveltyCount7D_std'].fillna(0,inplace=True)\n   # X['volumeCounts12H_min'].fillna(0,inplace=True)\n   # X['volumeCounts12H_max'].fillna(0,inplace=True)\n   # X['volumeCounts12H_mean'].fillna(0,inplace=True)\n   # X['volumeCounts12H_std'].fillna(0,inplace=True)\n   # X['volumeCounts24H_min'].fillna(0,inplace=True)\n   # X['volumeCounts24H_max'].fillna(0,inplace=True)\n   # X['volumeCounts24H_mean'].fillna(0,inplace=True)\n   # X['volumeCounts24H_std'].fillna(0,inplace=True)\n    #X['volumeCounts3D_min'].fillna(0,inplace=True)\n    #X['volumeCounts3D_max'].fillna(0,inplace=True)\n    #X['volumeCounts3D_mean'].fillna(0,inplace=True)\n    #X['volumeCounts3D_std'].fillna(0,inplace=True)\n    #X['volumeCounts5D_min'].fillna(0,inplace=True)\n    #X['volumeCounts5D_max'].fillna(0,inplace=True)\n    #X['volumeCounts5D_mean'].fillna(0,inplace=True)\n    #X['volumeCounts5D_std'].fillna(0,inplace=True)\n    #X['volumeCounts7D_min'].fillna(0,inplace=True)\n    #X['volumeCounts7D_max'].fillna(0,inplace=True)\n    #X['volumeCounts7D_mean'].fillna(0,inplace=True)\n    #X['volumeCounts7D_std'].fillna(0,inplace=True)\n    \n    #print(X.isnull().any())\n   # If not label-encoder... encode assetCode\n   \n    if le is None:\n        le_assetCode = label_encode(X['assetCode'], min_count=10)\n        le_assetName = label_encode(X['assetName'], min_count=5)\n    else:\n        # 'unpack' label encoders\n        le_assetCode, le_assetName = le\n        \n    X['assetCode'] = X['assetCode'].map(le_assetCode).fillna(-1).astype(np.int16)\n    X['assetName'] = X['assetName'].map(le_assetName).fillna(-1).astype(np.int16)\n    time_batch = X['time'].dt.day.astype(np.int8)\n    \n    try:\n        X.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n    except:\n        pass\n    try:\n        X.drop(columns=['universe'], inplace=True)\n    except:\n        pass\n    X['dayofweek'], X['month'] = X.time.dt.dayofweek.astype(np.int8), X.time.dt.month.astype(np.int8)\n    X.drop(columns='time', inplace=True)\n\n    # Fix some mixed-type columns\n    for bogus_col in ['marketCommentary_min', 'marketCommentary_max']:\n        X[bogus_col] = X[bogus_col].astype(np.float32)\n    \n    features = X[feature_cols]\n    return features, (le_assetCode, le_assetName),time_batch,universe",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5872402f1c0cbedcb84b25498c8faf3efcb0f59"
      },
      "cell_type": "code",
      "source": "def make_predictions(market_obs_df, news_obs_df, predictions_template_df):\n    \"\"\"\n    Predict confidence for one day and update predictions_template_df['confidenceValue']\n    @param market_obs_df: market_obs_df returned from env\n    @param predictions_template_df: predictions_template_df returned from env.\n    @return: None. prediction_template_df updated instead. \n    \"\"\"\n    # Preprocess the data\n    X, t, u = get_x_sub(market_obs_df, news_obs_df)\n    # Add look back window for LSTM, passing X only - we don't know y, we are predicting them\n    X = train_generator.with_look_back(X, None,t, u,look_back=90, look_back_step=10)\n    # Predict\n    y_pred = model.predict(X)\n    confidence_df=pd.DataFrame(y_pred*2-1, columns=['confidence'])\n\n    # Merge predicted confidence to predictions template\n    pred_df = pd.concat([predictions_template_df, confidence_df], axis=1).fillna(0)\n    predictions_template_df.confidenceValue = pred_df.confidence",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4cab3fe33ce4ffc87af1bbd807a904c0c07f59d7"
      },
      "cell_type": "code",
      "source": "days = env.get_prediction_days()\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    make_predictions(market_obs_df, news_obs_df, predictions_template_df)\n    env.predict(predictions_template_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7aba0816267c7bb8008723087f0dbac9fdba557f"
      },
      "cell_type": "code",
      "source": "env.write_submission_file()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5a6ff7da24617468a94a18c607e4fff51ebf3fe0"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}