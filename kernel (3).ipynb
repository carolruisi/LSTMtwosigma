{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport os.path\nimport random\nfrom pathlib import Path\nfrom time import time\nfrom itertools import chain\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import QuantileTransformer,StandardScaler, MinMaxScaler,OneHotEncoder, LabelEncoder, RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, LSTM, Embedding\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.utils import to_categorical\nimport tensorflow\nfrom kaggle.competitions import twosigmanews\n\n# Improve printed df readability\npd.options.display.float_format = '{:,.4f}'.format\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', 200)\n\nprint(os.listdir(\"../input\"))",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "['marketdata_sample.csv', 'news_sample.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e4c6d73d0bd15aa0a90ba67e81cac4f3884815c"
      },
      "cell_type": "code",
      "source": "env = twosigmanews.make_env()",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loading the data... This could take a minute.\nDone!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cdb8de212049feaee5ae7d97251e673167317a51"
      },
      "cell_type": "code",
      "source": "(market, news) = env.get_training_data()",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e5f7720c368c90479c2b9bfc857372d820328771"
      },
      "cell_type": "code",
      "source": "market_idx = market[['time', 'assetCode']]\nmarket_idx = market_idx.sample(3000000)\nmarket_idx = market_idx.sort_values(by=['time'])\nmarket_train_idx, market_test_idx = train_test_split(market_idx,test_size=0.1, shuffle=False, random_state=2018)\nmarket_train_idx, market_val_idx = train_test_split(market_train_idx, test_size=0.1, shuffle=False, random_state=2018)",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eafcd6a52d8e6375dd4d54fce2c41029d180454d"
      },
      "cell_type": "markdown",
      "source": "## Market feature extraction"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "85edd087f2e51454ba31d540d624004af2f9e288"
      },
      "cell_type": "code",
      "source": "class MarketPrepro:\n    assetcode_encoded = []\n    assetcode_train_count = 0\n    time_cols=['year', 'week', 'day', 'dayofweek']\n    numeric_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10']\n    feature_cols = ['assetCode_encoded']  + time_cols + numeric_cols\n    label_cols = ['returnsOpenNextMktres10']   \n    \n    def __init__(self):\n        self.cats={}\n        \n    def fit(self, market_train_df):\n        df = market_train_df.copy()\n        market_train_df = self.fix_train(market_train_df)\n        market_train_df = self.prepare_time_cols(market_train_df)\n        self.numeric_scaler = StandardScaler()\n        self.numeric_scaler.fit(market_train_df[self.numeric_cols + self.time_cols].astype(float))\n        market_train_df = self.encode_asset(market_train_df, True)\n        \n    def fix_train(self, train_df):\n        max_ratio  = 2\n        train_df = train_df[(train_df['close'] / train_df['open']).abs() <= max_ratio].loc[:]\n        train_df = self.safe_fix(train_df)\n        return(train_df)\n\n    def safe_fix(self, df):\n        df[self.numeric_cols] = df[ ['assetCode'] + self.numeric_cols].groupby('assetCode').transform(lambda g: g.fillna(method='bfill'))\n        df[self.numeric_cols] = df[self.numeric_cols].fillna(0)\n        df[self.numeric_cols] = df[self.numeric_cols].clip(df[self.numeric_cols].quantile(0.01), df[self.numeric_cols].quantile(0.99), axis=1)\n        return(df)\n    \n    def get_X(self,df):\n        df = df.copy()\n        df = self.safe_fix(df)\n        df = self.prepare_time_cols(df)\n        df = self.encode_asset(df, is_train=False)\n        df[self.numeric_cols+self.time_cols] = self.numeric_scaler.transform(df[self.numeric_cols+self.time_cols].astype(float))\n        return df[self.feature_cols]\n    \n    def get_y(self, df):\n        y=(df[self.label_cols] >=0).astype(float)\n        return y\n\n    def encode_asset(self, df, is_train):\n        def encode(assetcode):\n            try:\n                index_value = self.assetcode_encoded.index(assetcode) +1\n            except ValueError:\n                self.assetcode_encoded.append(assetcode)\n                index_value = len(self.assetcode_encoded)\n        if is_train:\n            self.assetcode_train_count = len(df['assetCode'].unique())+1\n        df['assetCode_encoded'] = df['assetCode'].apply(lambda assetcode: encode(assetcode))\n        return(df)\n        \n    def prepare_time_cols(self, df):\n        df = df.copy()\n        df['year'] = df['time'].dt.year\n        df['day'] = df['time'].dt.day\n        df['week'] = df['time'].dt.week\n        df['dayofweek'] = df['time'].dt.dayofweek\n        return(df)\n",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "01a1054deecd0e153800ad5903a3b9413e9fb5cf"
      },
      "cell_type": "code",
      "source": "market_prepro = MarketPrepro()",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4ba42638cd6509382582d2ef7b02711383474fb2"
      },
      "cell_type": "markdown",
      "source": "## News feature extraction"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "725ee92df319ffdba9d5db27ac68e08860f67f4a"
      },
      "cell_type": "code",
      "source": "class NewsPrepro:\n    news_cols_agg = {\n        'urgency': ['min', 'count'],\n        'takeSequence': ['max'],\n        'bodySize': ['min', 'max', 'mean', 'std'],\n        'wordCount': ['min', 'max', 'mean', 'std'],\n        'sentenceCount': ['min', 'max', 'mean', 'std'],\n        'companyCount': ['min', 'max', 'mean', 'std'],\n        'marketCommentary': ['min', 'max', 'mean', 'std'],\n        'relevance': ['min', 'max', 'mean', 'std'],\n        'sentimentNegative': ['min', 'max', 'mean', 'std'],\n        'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n        'sentimentPositive': ['min', 'max', 'mean', 'std'],\n        'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n        'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n        'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n        'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n        'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n        'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n        'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n        'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n        'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n        'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n        'volumeCounts7D': ['min', 'max', 'mean', 'std']\n            }\n    news_cols_numeric = set(news_cols_agg.keys()) - set(['assetCode', 'time'])\n        \n    def fit(self, news_train_df):\n        news_train_df = news_train_df.copy()\n        news_train_df_agg = self.aggregate_news(news_train_df)\n        news_train_df_agg.fillna(0, inplace=True)\n        self.numeric_scaler = StandardScaler()\n        self.numeric_scaler.fit(news_train_df_agg)\n        self.feature_cols = list(news_train_df_agg.columns.values)\n\n    def get_X(self, df):\n        news_df = df.copy()\n        news_df = self.aggregate_news(df)\n        news_df.fillna(0, inplace=True)\n        if not news_df.empty:\n            news_df_numeric = news_df._get_numeric_data().astype(float)\n            news_df[news_df_numeric.columns] = self.numeric_scaler.transform(news_df_numeric)\n        return(news_df)\n        \n    def aggregate_news(self, df):\n        df['assetCodes'] = df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n        if not df.empty: df.time = df.time.astype('datetime64[D, UTC]') \n        assetCodes_expanded = list(chain(*df['assetCodes']))\n        \n        if(not df.empty): assetCodes_index = df.index.repeat(df['assetCodes'].apply(len)) \n        else: assetCodes_index = df.index\n        assert len(assetCodes_index) == len(assetCodes_expanded)\n        df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n        news_cols = ['time', 'assetCodes'] + sorted(list(self.news_cols_agg.keys()))\n        df_expanded = pd.merge(df_assetCodes, df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n        df_aggregated = df_expanded.groupby(['time', 'assetCode']).agg(self.news_cols_agg)\n        df_aggregated.columns = ['_'.join(col).strip() for col in df_aggregated.columns.values]\n\n        return df_aggregated    ",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d030733704fc1a1da009c6862d383a2cc7d3b04c"
      },
      "cell_type": "code",
      "source": "news_prepro = NewsPrepro()",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2c1e3f03f25453f856bd7af014bfa632af750a60"
      },
      "cell_type": "markdown",
      "source": "**##Merge**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b84e6f57dba7ed270b14ec28888d9d537c0806a"
      },
      "cell_type": "code",
      "source": "class JoinedPreprocessor:\n    def __init__(self, market_prepro, news_prepro):\n        self.market_prepro = market_prepro\n        self.news_prepro = news_prepro\n        \n    def fit(self, market_train_idx, market, news):\n        market_train_df = market.loc[market_train_idx.index]\n        self.market_prepro.fit(market_train_df)\n        news_train_df = news.merge(market_train_idx, on=['time'])\n        self.news_prepro.fit(news_train_df)\n    \n    def get_X(self, market_df, news_df):\n        market_X = market_prepro.get_X(market_df)\n        market_X['time'] = market_df['time']\n        market_X['assetCode'] = market_df['assetCode']\n        news_X = news_prepro.get_X(news_df)\n        X = market_X.merge(news_X, how='left', left_on=['time', 'assetCode'], right_on=['time','assetCode'],  right_index=True)\n        X.fillna(0, inplace=True)\n        features = X[market_prepro.feature_cols + news_prepro.feature_cols]\n        return(features)\n\n    def get_y(self, market_df): \n        return(self.market_prepro.get_y(market_df))\n    \n    def get_Xy(self, market_df, news_df):\n        return(self.get_X(market_df, news_df), self.get_y(market_df))\n    \n    def fix_train(self, market_df, news_df):\n        return(market_prepro.fix_train(market_df), news_df)",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c0d0cefde3ea866a7f21a25b75dc8cec67f738f"
      },
      "cell_type": "code",
      "source": "prepro = JoinedPreprocessor(market_prepro, news_prepro)\nprepro.fit(market.loc[market_train_idx.index], market, news)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype bool, int8, float16, int16, float32, int32, int64, float64 were all converted to float64 by StandardScaler.\n  return self.partial_fit(X, y)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fe9e4d1c83f80d45cb7ef9baa4e3890ddf31f7e8"
      },
      "cell_type": "code",
      "source": "class JoinedGenerator:\n    def __init__(self, prepro, market, news, index_df):\n        self.market = market\n        self.prepro = prepro\n        self.news = news\n        self.index_df = index_df\n\n    def flow_lstm(self, batch_size, is_train, look_back, look_back_step):\n        while True:\n            batch_index_df = self.get_random_assets_idx(batch_size)\n            X, y = self.get_batch(batch_index_df, is_train)\n            X, y = self.with_look_back(X,y,look_back,look_back_step)\n            yield X,y\n    \n    def get_random_assets_idx(self, batch_size):\n        asset_codes = self.index_df['assetCode'].unique().tolist()\n\n        # Insert first asset\n        asset = np.random.choice(asset_codes)\n        asset_codes.remove(asset)\n        batch_index_df = self.index_df[self.index_df.assetCode == asset].tail(batch_size)\n        # Repeat until reach batch_size records\n        while (batch_index_df.index.size < batch_size) and (len(asset_codes) > 0):\n            asset = np.random.choice(asset_codes)\n            asset_codes.remove(asset)\n            asset_index_df = self.index_df[self.index_df.assetCode == asset].tail(batch_size - batch_index_df.index.size)\n            batch_index_df = pd.concat([batch_index_df, asset_index_df])\n        \n        return batch_index_df.sort_values(by=['assetCode', 'time'])\n            \n    def get_batch(self, batch_idx, is_train):\n        market_df = self.market.loc[batch_idx.index]\n        news_df = news.merge(batch_idx, on=['time'])\n        if is_train: \n            market_df, news_df = prepro.fix_train(market_df, news_df)\n        X = self.prepro.get_X(market_df, news_df)\n        y = self.prepro.get_y(market_df)\n        return(X, y)\n    \n    def with_look_back(self, X, y, look_back, look_back_step):\n        X_processed, y_processed = [], []\n        if look_back > len(X): \n            look_back = len(X)\n            look_back_step = min(look_back_step, look_back)\n            \n        for i in range(0,len(X)-look_back+1):\n            x_window = X.values[i:(i+look_back):look_back_step, :]\n            X_processed.append(x_window)\n            if y is None: continue\n            y_window = y.values[i+look_back-1, :]\n            y_processed.append(y_window)\n        if(y is not None): return np.array(X_processed), np.array(y_processed)\n        else: return np.array(X_processed)",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f5740df5e00ce6a57ce48e2847f4ffe9e4bee2af"
      },
      "cell_type": "code",
      "source": "join_generator = JoinedGenerator(prepro, market, news, market_train_idx)\nval_generator = JoinedGenerator(prepro, market, news, market_val_idx)",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7eae329a820e1614958e031fccc9e47c065d4abd",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "class lstm_tot:\n    look_back=90\n    look_back_step=10\n\n    def lstm_128():\n        model = Sequential()\n        input_size = len(market_prepro.feature_cols) + len(news_prepro.feature_cols)\n        model.add(LSTM(units=128, return_sequences=True, input_shape=(None,input_size)))\n        model.add(LSTM(units=64, return_sequences=True ))\n        model.add(LSTM(units=32, return_sequences=False))\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n        return(model)        \n\nmodel = lstm_tot.lstm_128()\nmodel.summary()\n",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_1 (LSTM)                (None, None, 128)         116736    \n_________________________________________________________________\nlstm_2 (LSTM)                (None, None, 64)          49408     \n_________________________________________________________________\nlstm_3 (LSTM)                (None, 32)                12416     \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 178,593\nTrainable params: 178,593\nNon-trainable params: 0\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "a06ff01bdad074a86dce8a3ef685f15eac017f4c"
      },
      "cell_type": "markdown",
      "source": "## Train market and news model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4e5814f7f8cbd1591c99e7580d16072ebf93961"
      },
      "cell_type": "code",
      "source": "batch_size=1000\nvalidation_batch_size=1000\nsteps_per_epoch=20\nvalidation_steps=5\nepochs=10\n\nprint(f'epochs:{epochs}, steps per epoch: {steps_per_epoch}, validation steps:{validation_steps}')\nprint(f'Batch_size:{batch_size}, validation batch size:{validation_batch_size}')\n\ntraining = model.fit_generator(join_generator.flow_lstm(batch_size=batch_size \n            , is_train=True \n            , look_back=lstm_tot.look_back \n            , look_back_step=lstm_tot.look_back_step) \n        , epochs=epochs \n        , validation_data=val_generator.flow_lstm(batch_size=validation_batch_size\n            , is_train=False\n            , look_back=lstm_tot.look_back\n            , look_back_step=lstm_tot.look_back_step) \n        , steps_per_epoch=steps_per_epoch \n        , validation_steps=validation_steps)",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "epochs:10, steps per epoch: 20, validation steps:5\nBatch_size:1000, validation batch size:1000\nEpoch 1/10\n20/20 [==============================] - 84s 4s/step - loss: 0.6926 - acc: 0.5351 - val_loss: 0.7017 - val_acc: 0.4918\nEpoch 2/10\n20/20 [==============================] - 75s 4s/step - loss: 0.6929 - acc: 0.5158 - val_loss: 0.7008 - val_acc: 0.4400\nEpoch 3/10\n20/20 [==============================] - 77s 4s/step - loss: 0.6928 - acc: 0.5134 - val_loss: 0.6910 - val_acc: 0.5438\nEpoch 4/10\n20/20 [==============================] - 78s 4s/step - loss: 0.6928 - acc: 0.5183 - val_loss: 0.6921 - val_acc: 0.5302\nEpoch 5/10\n20/20 [==============================] - 77s 4s/step - loss: 0.6927 - acc: 0.5121 - val_loss: 0.6945 - val_acc: 0.4749\nEpoch 6/10\n20/20 [==============================] - 77s 4s/step - loss: 0.6938 - acc: 0.4924 - val_loss: 0.6958 - val_acc: 0.4718\nEpoch 7/10\n20/20 [==============================] - 76s 4s/step - loss: 0.6933 - acc: 0.5019 - val_loss: 0.6939 - val_acc: 0.4959\nEpoch 8/10\n20/20 [==============================] - 77s 4s/step - loss: 0.6922 - acc: 0.5178 - val_loss: 0.6939 - val_acc: 0.4891\nEpoch 9/10\n20/20 [==============================] - 76s 4s/step - loss: 0.6930 - acc: 0.5146 - val_loss: 0.6917 - val_acc: 0.5164\nEpoch 10/10\n20/20 [==============================] - 75s 4s/step - loss: 0.6939 - acc: 0.4955 - val_loss: 0.6922 - val_acc: 0.5227\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "85468c59a89630d23b7d08bb809c44327d159ef4"
      },
      "cell_type": "code",
      "source": "market_df = market.loc[market_val_idx.index]\nnews_df = news.merge(market_val_idx, on=['time'])\nX,y = prepro.get_Xy(market_df, news_df)\nX,y = val_generator.with_look_back(X, y, look_back=lstm_tot.look_back, look_back_step=lstm_tot.look_back_step)\nconfidence_valid = np.array([model.predict(X)]) * 2 - 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4cb93a04dd4478a56c0c4b24568f4a9f770b5720"
      },
      "cell_type": "code",
      "source": "time = market_df['time'][lstm_tot.look_back-1]\nd_val = time.factorize()[0].tolist()\nr_val = market_df.returnsOpenNextMktres10.iloc[lstm_tot.look_back-1].tolist()\nu_val = market_df.universe.iloc[lstm_tot.look_back-1].tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a555747ec8864bf45d0988e835165f3b9671e7c"
      },
      "cell_type": "code",
      "source": "d_val = time.factorize()[0].tolist()\nr_val = market_df.returnsOpenNextMktres10.iloc[lstm_tot.look_back-1].tolist()\nu_val = market_df.universe.iloc[lstm_tot.look_back-1].tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "32733d377607e30095255a61bbe20387c339d950"
      },
      "cell_type": "code",
      "source": "x_t_i= (confidence_valid * r_val * u_val).tolist()\ndf = pd.DataFrame({'day': d_val,'col2': x_t_i}, index=market_val_idx.index)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_testcat = mean / std\nprint(f'CatBoost score: {score_testcat}')",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}